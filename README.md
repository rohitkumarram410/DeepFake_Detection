# ğŸ­ Deepfake Detection Using Hybrid Multi-Modal Synthetic-to-Real Image Mapping

## ğŸ“˜ Project Overview

This project proposes a novel and interpretable pipeline for deepfake detection by combining texture analysis, motion tracking, and physiological coherence in facial and non-facial regions. With the rise of highly photorealistic synthetic videos generated using GANs and neural face rendering, traditional detection methods often fall short in detecting subtle forgeries.

Our pipeline introduces a **multi-branch architecture** that not only analyzes pixel-level texture artifacts using **EfficientNet-B4**, but also captures **micro-muscle movements** through **optical flow estimation**, evaluates **facial symmetry**, and computes a novel **Neuro-Muscular Coherence Score (NMCS)** to understand the authenticity of expressions and facial dynamics. The goal is not just to detect, but to **explain**â€”highlighting manipulated zones and inconsistencies in motion.

---

## ğŸ§  Key Features

- ğŸ§© **Multi-Branch Deepfake Detection Pipeline**  
- ğŸ¯ **Zone-Wise Texture Artifact Detection** using EfficientNet-B4  
- ğŸ¥ **Micro-Muscle Optical Flow Estimation** with motion heatmaps  
- âš–ï¸ **Facial Symmetry & Expression Analysis** (e.g., blink balance, cheek lift)  
- ğŸ§  **Neuro-Muscular Coherence Score (NMCS)** for physiological consistency  
- ğŸ“Š **Explainable Predictions** with region-based inconsistency maps  
- ğŸ“ **Modular Training** and easy integration of custom datasets  

---

## ğŸ”§ Technologies & Tools

| Tool / Library              | Purpose                                                   |
|----------------------------|-----------------------------------------------------------|
| ğŸ Python 3.8+              | Core development language                                 |
| ğŸ”¥ PyTorch                  | Deep learning framework                                   |
| ğŸ§  EfficientNet-B4          | Feature extraction and texture analysis                   |
| ğŸ¯ MediaPipe                | Face detection and landmark tracking                      |
| ğŸï¸ Optical Flow (Farneback) | Temporal motion tracking and micro-expression estimation  |
| ğŸ“Š Matplotlib / Seaborn     | Plotting and performance visualization                    |
| ğŸ§ª pandas / PIL / OpenCV    | Data handling and image I/O                               |

---
## ğŸ› ï¸ Architecture

The detection pipeline consists of the following stages:

1. **Face Detection & Alignment** (via MediaPipe)
2. **Facial Zone Segmentation**
3. **Multi-Branch Feature Extraction:**
   - EfficientNet-based Texture Analysis
   - Optical Flow for Micro-Movements
   - NMCS computation for physiological coherence
4. **Feature Fusion**
5. **Classification Head** (Transformer)
6. **Output:** Probability Score + Explanation Map

---

## ğŸ§ª How It Works

1. **Input**: Short video clip or sequence of facial frames  
2. **Face Detection & Alignment**: Facial zones segmented using landmarks  
3. **Feature Extraction**: Parallel branches extract texture, motion, symmetry, and tension  
4. **NMCS Computation**: Measures coordination across facial zones  
5. **Fusion + Classification**: All features concatenated and passed through a classifier  
6. **Output**: Real / Fake prediction with explanation map  

---

## ğŸ“ˆ Results

- âœ… **Accuracy**: Achieved up to **93% accuracy** on Celeb-DF frame-level validation  
- ğŸ§  High interpretability with visual anomaly maps  
- ğŸ” Strong generalization across both facial and non-facial forgery zones  

---

## ğŸŒ Real-World Applications

- ğŸ“° **Media Integrity Verification**  
- ğŸ‘® **Digital Forensics**  
- ğŸ“¹ **Social Media Content Moderation**  
- ğŸ§ª **AI Model Evaluation and Research**  

---

## ğŸš€ Future Scope

- Integration of audio-lip sync mismatch analysis  
- Lightweight version for edge deployment (e.g., mobile forensic tools)  
- Adversarial robustness and domain adaptation across datasets  
