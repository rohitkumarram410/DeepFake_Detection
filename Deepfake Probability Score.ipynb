{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3310c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing NMCS from: D:\\DETECTION 1.0\\Celeb-DF\\aligned_faces_fake: 100%|██████████| 408/408 [00:00<00:00, 180915.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: D:\\DETECTION 1.0\\Celeb-DF\\nmcs_scores_fake.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dummy function for NMCS computation\n",
    "def compute_nmcs(video_folder):\n",
    "    # Example: return a single score and placeholder zone features\n",
    "    return {\n",
    "        \"video_id\": os.path.basename(video_folder),\n",
    "        \"nmcs_score\": np.random.rand(),\n",
    "        \"eyes_coord_var\": np.random.rand(),\n",
    "        \"mouth_coord_var\": np.random.rand(),\n",
    "        \"brow_coord_var\": np.random.rand(),\n",
    "    }\n",
    "\n",
    "def process_all_videos(base_path, label):\n",
    "    all_data = []\n",
    "    video_folders = sorted([os.path.join(base_path, d) for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))])\n",
    "    \n",
    "    for video_folder in tqdm(video_folders, desc=f\"Processing NMCS from: {base_path}\"):\n",
    "        result = compute_nmcs(video_folder)\n",
    "        result[\"label\"] = label\n",
    "        all_data.append(result)\n",
    "        \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# === Paths ===\n",
    "fake_base_path = r\"D:\\DETECTION 1.0\\Celeb-DF\\aligned_faces_fake\"\n",
    "nmcs_fake_output = r\"D:\\DETECTION 1.0\\Celeb-DF\\nmcs_scores_fake.csv\"\n",
    "\n",
    "# === Run processing ===\n",
    "nmcs_fake_df = process_all_videos(fake_base_path, label=1)\n",
    "nmcs_fake_df.to_csv(nmcs_fake_output, index=False)\n",
    "print(f\"✅ Saved: {nmcs_fake_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2455f6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Muscle from: D:\\DETECTION 1.0\\Celeb-DF\\aligned_faces_fake: 100%|██████████| 408/408 [00:00<00:00, 27143.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: D:\\DETECTION 1.0\\Celeb-DF\\muscle_tension_fake.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dummy muscle tension feature generator\n",
    "def compute_muscle_features(video_folder):\n",
    "    video_id = os.path.basename(video_folder)\n",
    "    features = {\n",
    "        \"video_id\": video_id,\n",
    "        \"label\": 1  # Fake\n",
    "    }\n",
    "    for i in range(35):  # Assuming 35 muscle tension features\n",
    "        features[f\"muscle_{i}\"] = np.random.rand()\n",
    "    return features\n",
    "\n",
    "def process_all_videos(base_path, label):\n",
    "    all_data = []\n",
    "    video_folders = sorted([os.path.join(base_path, d) for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))])\n",
    "    \n",
    "    for video_folder in tqdm(video_folders, desc=f\"Processing Muscle from: {base_path}\"):\n",
    "        result = compute_muscle_features(video_folder)\n",
    "        result[\"label\"] = label\n",
    "        all_data.append(result)\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# === Paths ===\n",
    "fake_base_path = r\"D:\\DETECTION 1.0\\Celeb-DF\\aligned_faces_fake\"\n",
    "muscle_fake_output = r\"D:\\DETECTION 1.0\\Celeb-DF\\muscle_tension_fake.csv\"\n",
    "\n",
    "# === Run processing ===\n",
    "muscle_fake_df = process_all_videos(fake_base_path, label=1)\n",
    "muscle_fake_df.to_csv(muscle_fake_output, index=False)\n",
    "print(f\"✅ Saved: {muscle_fake_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495e7f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Muscle from: D:\\DETECTION 1.0\\Celeb-DF\\aligned_faces_real: 100%|██████████| 408/408 [00:00<00:00, 37107.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: D:\\DETECTION 1.0\\Celeb-DF\\muscle_tension_real.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dummy muscle tension feature generator\n",
    "def compute_muscle_features(video_folder):\n",
    "    video_id = os.path.basename(video_folder)\n",
    "    features = {\n",
    "        \"video_id\": video_id,\n",
    "        \"label\": 1  # Fake\n",
    "    }\n",
    "    for i in range(35):  # Assuming 35 muscle tension features\n",
    "        features[f\"muscle_{i}\"] = np.random.rand()\n",
    "    return features\n",
    "\n",
    "def process_all_videos(base_path, label):\n",
    "    all_data = []\n",
    "    video_folders = sorted([os.path.join(base_path, d) for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))])\n",
    "    \n",
    "    for video_folder in tqdm(video_folders, desc=f\"Processing Muscle from: {base_path}\"):\n",
    "        result = compute_muscle_features(video_folder)\n",
    "        result[\"label\"] = label\n",
    "        all_data.append(result)\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# === Paths ===\n",
    "fake_base_path = r\"D:\\DETECTION 1.0\\Celeb-DF\\aligned_faces_real\"\n",
    "muscle_fake_output = r\"D:\\DETECTION 1.0\\Celeb-DF\\muscle_tension_real.csv\"\n",
    "\n",
    "# === Run processing ===\n",
    "muscle_fake_df = process_all_videos(fake_base_path, label=1)\n",
    "muscle_fake_df.to_csv(muscle_fake_output, index=False)\n",
    "print(f\"✅ Saved: {muscle_fake_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91786182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined NMCS saved to: D:\\DETECTION 1.0\\Celeb-DF\\nmcs_combined_scores.csv\n",
      "✅ Combined Muscle saved to: D:\\DETECTION 1.0\\Celeb-DF\\muscle_combined_tension.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Paths ===\n",
    "real_nmcs = r\"D:\\DETECTION 1.0\\Celeb-DF\\nmcs_scores_real.csv\"\n",
    "fake_nmcs = r\"D:\\DETECTION 1.0\\Celeb-DF\\nmcs_scores_fake.csv\"\n",
    "nmcs_out = r\"D:\\DETECTION 1.0\\Celeb-DF\\nmcs_combined_scores.csv\"\n",
    "\n",
    "real_muscle = r\"D:\\DETECTION 1.0\\Celeb-DF\\muscle_tension_real.csv\"\n",
    "fake_muscle = r\"D:\\DETECTION 1.0\\Celeb-DF\\muscle_tension_fake.csv\"\n",
    "muscle_out = r\"D:\\DETECTION 1.0\\Celeb-DF\\muscle_combined_tension.csv\"\n",
    "\n",
    "# === Combine NMCS ===\n",
    "nmcs_df = pd.concat([\n",
    "    pd.read_csv(real_nmcs),\n",
    "    pd.read_csv(fake_nmcs)\n",
    "], ignore_index=True)\n",
    "nmcs_df.to_csv(nmcs_out, index=False)\n",
    "print(f\"✅ Combined NMCS saved to: {nmcs_out}\")\n",
    "\n",
    "# === Combine Muscle ===\n",
    "muscle_df = pd.concat([\n",
    "    pd.read_csv(real_muscle),\n",
    "    pd.read_csv(fake_muscle)\n",
    "], ignore_index=True)\n",
    "muscle_df.to_csv(muscle_out, index=False)\n",
    "print(f\"✅ Combined Muscle saved to: {muscle_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e80de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Checking real/fake video_ids in each CSV:\n",
      "🔍 Motion CSV: Real Videos = 262, Fake Videos = 7\n",
      "🔍 NMCS CSV: Real Videos = 247, Fake Videos = 0\n",
      "🔍 Muscle CSV: Real Videos = 247, Fake Videos = 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "label_df = pd.read_csv(r\"D:\\DETECTION 1.0\\Celeb-DF\\aligned_frame_labels.csv\")\n",
    "motion_df = pd.read_csv(r\"D:\\DETECTION 1.0\\Celeb-DF\\combined_motion.csv\")\n",
    "nmcs_df = pd.read_csv(r\"D:\\DETECTION 1.0\\Celeb-DF\\nmcs_combined_scores.csv\")\n",
    "muscle_df = pd.read_csv(r\"D:\\DETECTION 1.0\\Celeb-DF\\muscle_combined_tension.csv\")\n",
    "\n",
    "# Normalize video_id\n",
    "label_df['video_id'] = label_df['video_id'].astype(str).apply(lambda x: x.split('_')[0])\n",
    "motion_df['video_id'] = motion_df['video_id'].astype(str).apply(lambda x: x.split('_')[0])\n",
    "nmcs_df['video_id'] = nmcs_df['video_id'].astype(str)\n",
    "muscle_df['video_id'] = muscle_df['video_id'].astype(str)\n",
    "\n",
    "# Get sets of real/fake video_ids from label_df\n",
    "real_ids = set(label_df[label_df['label'] == 0]['video_id'])\n",
    "fake_ids = set(label_df[label_df['label'] == 1]['video_id'])\n",
    "\n",
    "# Function to count overlap\n",
    "def count_overlap(df, name):\n",
    "    video_ids = set(df['video_id'])\n",
    "    real_in = len(real_ids & video_ids)\n",
    "    fake_in = len(fake_ids & video_ids)\n",
    "    print(f\"🔍 {name} CSV: Real Videos = {real_in}, Fake Videos = {fake_in}\")\n",
    "\n",
    "# Count real/fake in each CSV\n",
    "print(\"\\n🎯 Checking real/fake video_ids in each CSV:\")\n",
    "count_overlap(motion_df, \"Motion\")\n",
    "count_overlap(nmcs_df, \"NMCS\")\n",
    "count_overlap(muscle_df, \"Muscle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b9a954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing REAL: 100%|██████████| 408/408 [28:42<00:00,  4.22s/it]\n",
      "Processing FAKE: 100%|██████████| 408/408 [28:04<00:00,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files saved: nmcs_features_naya.csv and muscle_features_naya.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "FAKE_PATH = r\"D:\\DETECTION 1.0\\Celeb-DF\\aligned_faces_fake\"\n",
    "REAL_PATH = r\"D:\\DETECTION 1.0\\Celeb-DF\\aligned_faces_real\"\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Define facial zone landmark pairs\n",
    "zone_pairs = [\n",
    "    (10, 152),  # forehead-chin\n",
    "    (33, 263),  # left eye - right eye\n",
    "    (61, 291),  # left mouth corner - right mouth corner\n",
    "    (234, 454), # left cheek - right cheek\n",
    "    (1, 199),   # nose bridge\n",
    "]\n",
    "\n",
    "def extract_landmarks(image):\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_face_landmarks:\n",
    "        return [(lm.x, lm.y) for lm in results.multi_face_landmarks[0].landmark]\n",
    "    return None\n",
    "\n",
    "def compute_muscle_tension(landmarks):\n",
    "    tensions = []\n",
    "    for (i, j) in zone_pairs:\n",
    "        xi, yi = landmarks[i]\n",
    "        xj, yj = landmarks[j]\n",
    "        dist = np.sqrt((xi - xj)**2 + (yi - yj)**2)\n",
    "        tensions.append(dist)\n",
    "    return tensions\n",
    "\n",
    "def compute_nmcs(tension_sequence):\n",
    "    return np.std(tension_sequence, axis=0)\n",
    "\n",
    "def process_video_frames(video_folder):\n",
    "    tension_sequence = []\n",
    "    frames = sorted(os.listdir(video_folder))\n",
    "    for frame_file in frames:\n",
    "        img_path = os.path.join(video_folder, frame_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        landmarks = extract_landmarks(img)\n",
    "        if landmarks:\n",
    "            tension = compute_muscle_tension(landmarks)\n",
    "            tension_sequence.append(tension)\n",
    "    if len(tension_sequence) < 2:\n",
    "        return None, None\n",
    "    tension_sequence = np.array(tension_sequence)\n",
    "    nmcs = compute_nmcs(tension_sequence)\n",
    "    avg_tension = np.mean(tension_sequence, axis=0)\n",
    "    return nmcs, avg_tension\n",
    "\n",
    "def process_dataset(folder_path, label):\n",
    "    nmcs_rows = []\n",
    "    tension_rows = []\n",
    "    for folder in tqdm(os.listdir(folder_path), desc=f\"Processing {'FAKE' if label==1 else 'REAL'}\"):\n",
    "        video_folder = os.path.join(folder_path, folder)\n",
    "        if not os.path.isdir(video_folder):\n",
    "            continue\n",
    "        nmcs, tension = process_video_frames(video_folder)\n",
    "        if nmcs is not None:\n",
    "            nmcs_row = [folder] + list(nmcs) + [label]\n",
    "            tension_row = [folder] + list(tension) + [label]\n",
    "            nmcs_rows.append(nmcs_row)\n",
    "            tension_rows.append(tension_row)\n",
    "    return nmcs_rows, tension_rows\n",
    "\n",
    "# Feature column names\n",
    "nmcs_cols = [\"video\"] + [f\"nmcs_{i}\" for i in range(len(zone_pairs))] + [\"label\"]\n",
    "tension_cols = [\"video\"] + [f\"tension_{i}\" for i in range(len(zone_pairs))] + [\"label\"]\n",
    "\n",
    "# Process real and fake data\n",
    "nmcs_real, tension_real = process_dataset(REAL_PATH, label=0)\n",
    "nmcs_fake, tension_fake = process_dataset(FAKE_PATH, label=1)\n",
    "\n",
    "# Combine and save\n",
    "df_nmcs = pd.DataFrame(nmcs_real + nmcs_fake, columns=nmcs_cols)\n",
    "df_tension = pd.DataFrame(tension_real + tension_fake, columns=tension_cols)\n",
    "\n",
    "df_nmcs.to_csv(\"nmcs_features_naya.csv\", index=False)\n",
    "df_tension.to_csv(\"muscle_features_naya.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files saved: nmcs_features_naya.csv and muscle_features_naya.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a5f1598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing REAL: 100%|██████████| 408/408 [26:41<00:00,  3.93s/it]\n",
      "Processing FAKE: 100%|██████████| 408/408 [28:10<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ motion_features.csv saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to real and fake aligned face videos\n",
    "FAKE_PATH = r\"D:\\DETECTION 1.0\\Celeb-DF\\aligned_faces_fake\"\n",
    "REAL_PATH = r\"D:\\DETECTION 1.0\\Celeb-DF\\aligned_faces_real\"\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Define landmark pairs for facial zones\n",
    "zone_pairs = [\n",
    "    (10, 152),  # Forehead ↔ Chin\n",
    "    (33, 263),  # Eye corners\n",
    "    (61, 291),  # Mouth corners\n",
    "    (234, 454), # Cheekbones\n",
    "    (1, 199),   # Nose bridge\n",
    "]\n",
    "\n",
    "def extract_landmarks(image):\n",
    "    \"\"\"Extract normalized facial landmarks using MediaPipe.\"\"\"\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_face_landmarks:\n",
    "        return [(lm.x, lm.y) for lm in results.multi_face_landmarks[0].landmark]\n",
    "    return None\n",
    "\n",
    "def compute_zone_distances(landmarks):\n",
    "    \"\"\"Compute distances between defined landmark pairs.\"\"\"\n",
    "    distances = []\n",
    "    for (i, j) in zone_pairs:\n",
    "        xi, yi = landmarks[i]\n",
    "        xj, yj = landmarks[j]\n",
    "        dist = np.sqrt((xi - xj) ** 2 + (yi - yj) ** 2)\n",
    "        distances.append(dist)\n",
    "    return distances\n",
    "\n",
    "def compute_motion(tension_sequence):\n",
    "    \"\"\"Compute mean inter-frame motion magnitude.\"\"\"\n",
    "    diffs = np.diff(tension_sequence, axis=0)\n",
    "    motion = np.mean(np.abs(diffs), axis=0)\n",
    "    return motion\n",
    "\n",
    "def process_video_frames(video_folder):\n",
    "    \"\"\"Process all frames in a folder and compute motion features.\"\"\"\n",
    "    tension_sequence = []\n",
    "    frames = sorted(os.listdir(video_folder))\n",
    "    for frame_file in frames:\n",
    "        frame_path = os.path.join(video_folder, frame_file)\n",
    "        image = cv2.imread(frame_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "        landmarks = extract_landmarks(image)\n",
    "        if landmarks:\n",
    "            tensions = compute_zone_distances(landmarks)\n",
    "            tension_sequence.append(tensions)\n",
    "    if len(tension_sequence) < 2:\n",
    "        return None  # Not enough frames\n",
    "    tension_sequence = np.array(tension_sequence)\n",
    "    motion_features = compute_motion(tension_sequence)\n",
    "    return motion_features\n",
    "\n",
    "def process_dataset(folder_path, label):\n",
    "    \"\"\"Process all video folders in a dataset and return motion features.\"\"\"\n",
    "    motion_rows = []\n",
    "    for folder_name in tqdm(os.listdir(folder_path), desc=f\"Processing {'FAKE' if label == 1 else 'REAL'}\"):\n",
    "        video_folder = os.path.join(folder_path, folder_name)\n",
    "        if not os.path.isdir(video_folder):\n",
    "            continue\n",
    "        motion = process_video_frames(video_folder)\n",
    "        if motion is not None:\n",
    "            row = [folder_name] + list(motion) + [label]\n",
    "            motion_rows.append(row)\n",
    "    return motion_rows\n",
    "\n",
    "# Define column names\n",
    "motion_cols = [\"video\"] + [f\"motion_{i}\" for i in range(len(zone_pairs))] + [\"label\"]\n",
    "\n",
    "# Process real and fake datasets\n",
    "motion_real = process_dataset(REAL_PATH, label=0)\n",
    "motion_fake = process_dataset(FAKE_PATH, label=1)\n",
    "\n",
    "# Combine and save\n",
    "df_motion = pd.DataFrame(motion_real + motion_fake, columns=motion_cols)\n",
    "df_motion.to_csv(\"motion_features_naya.csv\", index=False)\n",
    "\n",
    "print(\"✅ motion_features.csv saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5feb797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion Features Columns: Index(['video_id', 'motion_0', 'motion_1', 'motion_2', 'motion_3', 'motion_4',\n",
      "       'label'],\n",
      "      dtype='object')\n",
      "Aligned Features Columns: Index(['video_id', 'filepath', 'label'], dtype='object')\n",
      "Fused DataFrame shape: (208643, 7)\n",
      "   video_id  motion_0  motion_1  motion_2  motion_3  motion_4  label\n",
      "0  id0_0000  0.002638  0.001993  0.001686  0.003293  0.001775      0\n",
      "1  id0_0000  0.002638  0.001993  0.001686  0.003293  0.001775      0\n",
      "2  id0_0000  0.002638  0.001993  0.001686  0.003293  0.001775      0\n",
      "3  id0_0000  0.002638  0.001993  0.001686  0.003293  0.001775      0\n",
      "4  id0_0000  0.002638  0.001993  0.001686  0.003293  0.001775      0\n",
      "\n",
      "Label Distribution:\n",
      "label\n",
      "1    153637\n",
      "0     55006\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label Meaning:\n",
      "0 = Real\n",
      "1 = Fake\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSVs\n",
    "motion_df = pd.read_csv(r\"D:\\DETECTION 1.0\\Celeb-DF\\motion_features_naya.csv\")\n",
    "aligned_df = pd.read_csv(r\"D:\\DETECTION 1.0\\Celeb-DF\\aligned_frame_labels.csv\")\n",
    "\n",
    "# Preview columns\n",
    "print(\"Motion Features Columns:\", motion_df.columns)\n",
    "print(\"Aligned Features Columns:\", aligned_df.columns)\n",
    "\n",
    "# Merge on 'video_id'\n",
    "fused_df = pd.merge(motion_df, aligned_df[['video_id', 'label']], on='video_id', how='inner')\n",
    "\n",
    "# Handle duplicate label columns if they exist\n",
    "if 'label_x' in fused_df.columns and 'label_y' in fused_df.columns:\n",
    "    fused_df = fused_df.drop(columns=['label_x'])                # Drop label_x\n",
    "    fused_df.rename(columns={'label_y': 'label'}, inplace=True)  # Rename label_y → label\n",
    "\n",
    "# Save to CSV\n",
    "fused_df.to_csv('fused_motion_aligned_features.csv', index=False)\n",
    "\n",
    "# Final preview\n",
    "print(f\"Fused DataFrame shape: {fused_df.shape}\")\n",
    "print(fused_df.head())\n",
    "\n",
    "# Count real and fake labels\n",
    "label_counts = fused_df['label'].value_counts()\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Optional: Label names if 0 = real, 1 = fake\n",
    "print(\"\\nLabel Meaning:\\n0 = Real\\n1 = Fake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6dd1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\envs\\torch_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged shape: (310828, 7)\n",
      "📎 Columns: Index(['video_id', 'motion_0', 'motion_1', 'motion_2', 'motion_3', 'motion_4',\n",
      "       'label'],\n",
      "      dtype='object')\n",
      "🧪 Total features: 5 → zone_count: 5, zone_dim: 1\n",
      "🚀 Starting training with input_dim=1, zone_count=5, device=cuda\n",
      "📦 Epoch 01: Loss=6512.5619, Accuracy=0.5673\n",
      "📦 Epoch 02: Loss=6388.1853, Accuracy=0.5846\n",
      "📦 Epoch 03: Loss=6271.0454, Accuracy=0.6017\n",
      "📦 Epoch 04: Loss=6145.1993, Accuracy=0.6134\n",
      "📦 Epoch 05: Loss=6063.7661, Accuracy=0.6222\n",
      "📦 Epoch 06: Loss=5992.7052, Accuracy=0.6330\n",
      "📦 Epoch 07: Loss=5899.6051, Accuracy=0.6444\n",
      "📦 Epoch 08: Loss=5812.5275, Accuracy=0.6521\n",
      "📦 Epoch 09: Loss=5727.6522, Accuracy=0.6581\n",
      "📦 Epoch 10: Loss=5650.4568, Accuracy=0.6633\n",
      "📦 Epoch 11: Loss=5535.6223, Accuracy=0.6718\n",
      "📦 Epoch 12: Loss=5433.0187, Accuracy=0.6772\n",
      "📦 Epoch 13: Loss=5329.3873, Accuracy=0.6855\n",
      "📦 Epoch 14: Loss=5234.6558, Accuracy=0.6929\n",
      "📦 Epoch 15: Loss=5153.3289, Accuracy=0.6980\n",
      "📦 Epoch 16: Loss=5066.1068, Accuracy=0.7050\n",
      "📦 Epoch 17: Loss=4985.8821, Accuracy=0.7114\n",
      "📦 Epoch 18: Loss=4920.5807, Accuracy=0.7158\n",
      "📦 Epoch 19: Loss=4839.6680, Accuracy=0.7230\n",
      "📦 Epoch 20: Loss=4759.5571, Accuracy=0.7291\n",
      "📦 Epoch 21: Loss=4697.3337, Accuracy=0.7340\n",
      "📦 Epoch 22: Loss=4635.9726, Accuracy=0.7373\n",
      "📦 Epoch 23: Loss=4566.0543, Accuracy=0.7433\n",
      "📦 Epoch 24: Loss=4514.0549, Accuracy=0.7461\n",
      "📦 Epoch 25: Loss=4461.5021, Accuracy=0.7496\n",
      "📦 Epoch 26: Loss=4413.2043, Accuracy=0.7540\n",
      "📦 Epoch 27: Loss=4358.5350, Accuracy=0.7572\n",
      "📦 Epoch 28: Loss=4315.0086, Accuracy=0.7604\n",
      "📦 Epoch 29: Loss=4272.2361, Accuracy=0.7631\n",
      "📦 Epoch 30: Loss=4222.2498, Accuracy=0.7664\n",
      "📦 Epoch 31: Loss=4189.7527, Accuracy=0.7698\n",
      "📦 Epoch 32: Loss=4151.1941, Accuracy=0.7722\n",
      "📦 Epoch 33: Loss=4104.6181, Accuracy=0.7753\n",
      "📦 Epoch 34: Loss=4068.3009, Accuracy=0.7780\n",
      "📦 Epoch 35: Loss=4038.5542, Accuracy=0.7799\n",
      "📦 Epoch 36: Loss=3987.0723, Accuracy=0.7844\n",
      "📦 Epoch 37: Loss=3958.2896, Accuracy=0.7862\n",
      "📦 Epoch 38: Loss=3914.2803, Accuracy=0.7884\n",
      "📦 Epoch 39: Loss=3886.9312, Accuracy=0.7904\n",
      "📦 Epoch 40: Loss=3841.4463, Accuracy=0.7939\n",
      "📦 Epoch 41: Loss=3811.3978, Accuracy=0.7948\n",
      "📦 Epoch 42: Loss=3778.6767, Accuracy=0.7972\n",
      "📦 Epoch 43: Loss=3744.8593, Accuracy=0.7998\n",
      "📦 Epoch 44: Loss=3718.5916, Accuracy=0.8016\n",
      "📦 Epoch 45: Loss=3678.8718, Accuracy=0.8043\n",
      "📦 Epoch 46: Loss=3662.3918, Accuracy=0.8052\n",
      "📦 Epoch 47: Loss=3618.2834, Accuracy=0.8080\n",
      "📦 Epoch 48: Loss=3592.8293, Accuracy=0.8095\n",
      "📦 Epoch 49: Loss=3568.1917, Accuracy=0.8113\n",
      "📦 Epoch 50: Loss=3533.6934, Accuracy=0.8132\n",
      "📦 Epoch 51: Loss=3501.8641, Accuracy=0.8161\n",
      "📦 Epoch 52: Loss=3473.9629, Accuracy=0.8185\n",
      "📦 Epoch 53: Loss=3454.4199, Accuracy=0.8201\n",
      "📦 Epoch 54: Loss=3444.4855, Accuracy=0.8198\n",
      "📦 Epoch 55: Loss=3408.2120, Accuracy=0.8236\n",
      "📦 Epoch 56: Loss=3387.6295, Accuracy=0.8247\n",
      "📦 Epoch 57: Loss=3362.9923, Accuracy=0.8262\n",
      "📦 Epoch 58: Loss=3338.3417, Accuracy=0.8280\n",
      "📦 Epoch 59: Loss=3330.2041, Accuracy=0.8287\n",
      "📦 Epoch 60: Loss=3295.2186, Accuracy=0.8309\n",
      "📦 Epoch 61: Loss=3286.5323, Accuracy=0.8315\n",
      "📦 Epoch 62: Loss=3266.4609, Accuracy=0.8329\n",
      "📦 Epoch 63: Loss=3243.3432, Accuracy=0.8341\n",
      "📦 Epoch 64: Loss=3224.7906, Accuracy=0.8356\n",
      "📦 Epoch 65: Loss=3196.2169, Accuracy=0.8377\n",
      "📦 Epoch 66: Loss=3186.5169, Accuracy=0.8378\n",
      "📦 Epoch 67: Loss=3171.9074, Accuracy=0.8386\n",
      "📦 Epoch 68: Loss=3148.4869, Accuracy=0.8401\n",
      "📦 Epoch 69: Loss=3128.2220, Accuracy=0.8412\n",
      "📦 Epoch 70: Loss=3103.9670, Accuracy=0.8428\n",
      "📦 Epoch 71: Loss=3092.1900, Accuracy=0.8437\n",
      "📦 Epoch 72: Loss=3074.7192, Accuracy=0.8444\n",
      "📦 Epoch 73: Loss=3065.9857, Accuracy=0.8454\n",
      "📦 Epoch 74: Loss=3053.1265, Accuracy=0.8456\n",
      "📦 Epoch 75: Loss=3043.7492, Accuracy=0.8463\n",
      "📦 Epoch 76: Loss=3026.1191, Accuracy=0.8472\n",
      "📦 Epoch 77: Loss=3013.8008, Accuracy=0.8481\n",
      "📦 Epoch 78: Loss=2994.5894, Accuracy=0.8488\n",
      "📦 Epoch 79: Loss=2977.6436, Accuracy=0.8499\n",
      "📦 Epoch 80: Loss=2959.3622, Accuracy=0.8515\n",
      "📦 Epoch 81: Loss=2957.0385, Accuracy=0.8515\n",
      "📦 Epoch 82: Loss=2937.9959, Accuracy=0.8524\n",
      "📦 Epoch 83: Loss=2931.5735, Accuracy=0.8529\n",
      "📦 Epoch 84: Loss=2902.4988, Accuracy=0.8550\n",
      "📦 Epoch 85: Loss=2896.4879, Accuracy=0.8549\n",
      "📦 Epoch 86: Loss=2883.2212, Accuracy=0.8561\n",
      "📦 Epoch 87: Loss=2872.2504, Accuracy=0.8567\n",
      "📦 Epoch 88: Loss=2860.2998, Accuracy=0.8567\n",
      "📦 Epoch 89: Loss=2859.0538, Accuracy=0.8566\n",
      "📦 Epoch 90: Loss=2846.2045, Accuracy=0.8575\n",
      "📦 Epoch 91: Loss=2824.6474, Accuracy=0.8591\n",
      "📦 Epoch 92: Loss=2809.9625, Accuracy=0.8597\n",
      "📦 Epoch 93: Loss=2798.1627, Accuracy=0.8600\n",
      "📦 Epoch 94: Loss=2796.1745, Accuracy=0.8609\n",
      "📦 Epoch 95: Loss=2780.8264, Accuracy=0.8613\n",
      "📦 Epoch 96: Loss=2768.1484, Accuracy=0.8616\n",
      "📦 Epoch 97: Loss=2759.7932, Accuracy=0.8625\n",
      "📦 Epoch 98: Loss=2737.2050, Accuracy=0.8634\n",
      "📦 Epoch 99: Loss=2728.6663, Accuracy=0.8644\n",
      "📦 Epoch 100: Loss=2732.0525, Accuracy=0.8640\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- CONFIG ---\n",
    "motion_csv = r\"D:/DETECTION 1.0/Celeb-DF/motion_features_naya.csv\"\n",
    "label_csv  = r\"D:/DETECTION 1.0/Celeb-DF/aligned_frame_labels.csv\"\n",
    "batch_size = 32\n",
    "epochs     = 100\n",
    "lr         = 1e-4\n",
    "num_classes = 2\n",
    "zone_count = 5   # As inferred from earlier: 5 motion features → 5 zones\n",
    "\n",
    "# --- DATASET ---\n",
    "class MotionFeatureDataset(Dataset):\n",
    "    def __init__(self, motion_path, label_path):\n",
    "        motion_df = pd.read_csv(motion_path)\n",
    "        label_df = pd.read_csv(label_path)\n",
    "\n",
    "        # Ensure correct ID formatting\n",
    "        motion_df['video_id'] = motion_df['video_id'].astype(str).str.zfill(5)\n",
    "\n",
    "        # Remove existing label if present in motion_df\n",
    "        if 'label' in motion_df.columns:\n",
    "            motion_df = motion_df.drop(columns=['label'])\n",
    "\n",
    "        # Merge\n",
    "        df = pd.merge(motion_df, label_df[['video_id', 'label']], on='video_id', how='inner')\n",
    "        print(f\"✅ Merged shape: {df.shape}\")\n",
    "        print(f\"📎 Columns: {df.columns}\")\n",
    "\n",
    "        # Handle label_x / label_y case\n",
    "        if 'label_y' in df.columns:\n",
    "            df = df.drop(columns=['label_x'])\n",
    "            df = df.rename(columns={'label_y': 'label'})\n",
    "\n",
    "        if 'label' not in df.columns:\n",
    "            raise ValueError(\"❌ 'label' column missing after merge!\")\n",
    "\n",
    "        self.labels = df['label'].values.astype('long')\n",
    "        self.features = df.drop(columns=['video_id', 'label'])\n",
    "\n",
    "        # Normalize\n",
    "        scaler = StandardScaler()\n",
    "        self.features = pd.DataFrame(scaler.fit_transform(self.features), columns=self.features.columns)\n",
    "\n",
    "        self.zone_count = 5\n",
    "        self.zone_dim = self.features.shape[1] // self.zone_count\n",
    "        print(f\"🧪 Total features: {self.features.shape[1]} → zone_count: {self.zone_count}, zone_dim: {self.zone_dim}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat = self.features.iloc[idx].values.astype('float32')\n",
    "        feat = feat.reshape(self.zone_count, self.zone_dim)\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(feat), torch.tensor(label)\n",
    "\n",
    "# --- MODEL ---\n",
    "class LightTransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=1, num_layers=2, num_classes=2, proj_dim=16):\n",
    "        super().__init__()\n",
    "        self.project = nn.Linear(input_dim, proj_dim)  # project 1D → higher dim\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=proj_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(proj_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):  # [B, Zones, input_dim]\n",
    "        x = self.project(x)     # [B, Zones, proj_dim]\n",
    "        x = self.encoder(x)     # [B, Zones, proj_dim]\n",
    "        x = x.mean(dim=1)       # Mean pooling across zones\n",
    "        return self.classifier(x)\n",
    "\n",
    "# --- PREPARE DATA & MODEL ---\n",
    "dataset = MotionFeatureDataset(motion_csv, label_csv)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = dataset.zone_dim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Starting training with input_dim={input_dim}, zone_count={zone_count}, device={device}\")\n",
    "\n",
    "model = LightTransformerClassifier(input_dim=input_dim, num_heads=1, num_layers=2, num_classes=num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device).long()  # Make sure y is long\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "\n",
    "    acc = correct / len(dataset)\n",
    "    print(f\"📦 Epoch {epoch+1:02}: Loss={total_loss:.4f}, Accuracy={acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29391cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"light_transformer_motion.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c1b368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LightTransformerClassifier(\n",
       "  (project): Linear(in_features=1, out_features=16, bias=True)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model for inference\n",
    "model = LightTransformerClassifier(input_dim=dataset.zone_dim, num_heads=2, num_layers=2, num_classes=2).to(device)\n",
    "model.load_state_dict(torch.load(\"light_transformer_motion.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8adbbd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load your full motion feature file\n",
    "motion_df = pd.read_csv(r\"D:/DETECTION 1.0/Celeb-DF/motion_features_naya.csv\")\n",
    "\n",
    "# Pick a video row (change the index as needed)\n",
    "sample_row = motion_df.iloc[0]\n",
    "\n",
    "# Extract features only\n",
    "video_id = sample_row['video_id']\n",
    "feature_values = sample_row.drop('video_id').values.astype('float32')\n",
    "\n",
    "# Reshape to match model input: [1, zones, zone_dim]\n",
    "zone_count = 6\n",
    "zone_dim = len(feature_values) // zone_count\n",
    "sample_tensor = torch.tensor(feature_values).reshape(1, zone_count, zone_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b2eb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(sample_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "    confidence = torch.softmax(output, dim=1).max().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89aa9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 Video ID: 0 → Predicted: REAL ✅ (0.54 confidence)\n"
     ]
    }
   ],
   "source": [
    "if predicted_class == 0:\n",
    "    print(f\"🟢 Video ID: {video_id} → Predicted: REAL ✅ ({confidence:.2f} confidence)\")\n",
    "else:\n",
    "    print(f\"🔴 Video ID: {video_id} → Predicted: FAKE ❌ ({confidence:.2f} confidence)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847b0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
